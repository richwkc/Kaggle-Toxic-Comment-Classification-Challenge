{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec then RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "\n",
    "importDirectory = \"../state/data/preprocessed-train-test/\"\n",
    "\n",
    "train, test, data, contestTest = map(\n",
    "    lambda filename: pd.read_csv(path.join(importDirectory, filename)), \n",
    "    [\"train.csv\", \"test.csv\", \"all.csv\", \"contest-test.csv\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (127656, 8), test: (31915, 8), all: (159571, 8), contestTest: (153164, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"train: {}, test: {}, all: {}, contestTest: {}\".format(\n",
    "    train.shape, test.shape, data.shape, contestTest.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitSentences(dataset):\n",
    "    return (dataset.comment_text\n",
    "    .str.replace(\"[^A-Za-z\\s]\", \"\")\n",
    "    .str.lower()\n",
    "    .str.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.92 s, sys: 168 ms, total: 3.09 s\n",
      "Wall time: 3.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "splitTrain = splitSentences(train)\n",
    "splitTest = splitSentences(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert words to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "allWords = set([word for sentence in pd.concat([splitTrain, splitTest]) for word in sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordToInteger = { word: index for index, word in enumerate(allWords) }\n",
    "integerToWord = { index: word for index, word in enumerate(allWords) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "integerTrain = splitTrain.apply(lambda sentence: [wordToInteger[word] for word in sentence])\n",
    "integerTest = splitTest.apply(lambda sentence: [wordToInteger[word] for word in sentence])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Int2Vec embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "numDimensions = 50\n",
    "maxSeqLength = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.6 s, sys: 360 ms, total: 22.9 s\n",
      "Wall time: 23.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import gensim\n",
    "\n",
    "w2vModel = gensim.models.KeyedVectors.load_word2vec_format(\"../state/external-models/glove.6B/w2v.glove.6B.50.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "availableWords = set.intersection(allWords, set(w2vModel.vocab.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(432432)\n",
    "\n",
    "int2vec = {index: w2vModel.word_vec(word) \n",
    "             if word in availableWords \n",
    "             else np.random.normal(scale=.644, size=(numDimensions,))\n",
    "         for index, word in integerToWord.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddingMatrix = np.array([vector for vector in int2vec.values()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oversample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oversample(dataset, label): \n",
    "    multiples = int(dataset[dataset[label] == 0].shape[0] / dataset[dataset[label] == 1].shape[0])\n",
    "    \n",
    "    datasetPositive = dataset[dataset[label] == 1]\n",
    "    \n",
    "    return pd.concat([dataset] + multiples * [datasetPositive]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainOversampled = oversample(train, \"toxic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero pad vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padArrayWithZeros(array):\n",
    "    fullArray = np.zeros(maxSeqLength)\n",
    "    fullArray[:min(array.shape[0], maxSeqLength)] = array[:min(array.shape[0], maxSeqLength)]\n",
    "    return fullArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareText(dataset):\n",
    "    return np.array(splitSentences(dataset)\n",
    "        .apply(lambda sentence: \n",
    "            padArrayWithZeros(np.array([\n",
    "                wordToInteger[word]\n",
    "                for word in sentence])))\n",
    "        .tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.3 s, sys: 492 ms, total: 10.8 s\n",
      "Wall time: 10.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainSentences = prepareText(trainOversampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLabels = np.array(trainOversampled\n",
    "    .toxic\n",
    "    .apply(lambda label: np.array([0, 1]) if label == 1 else np.array([1, 0]))\n",
    "    .tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "testSentences = prepareText(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "testLabels = np.array(test\n",
    "    .toxic\n",
    "    .apply(lambda label: np.array([0, 1]) if label == 1 else np.array([1, 0]))\n",
    "    .tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small datasets\n",
    "smallSize = 10000\n",
    "trainSentences = trainSentences[:smallSize]\n",
    "trainLabels = trainLabels[:smallSize]\n",
    "testSentences = trainSentences[:smallSize]\n",
    "testLabels = trainLabels[:smallSize]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LMST RNN with keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstmUnits = [100]\n",
    "numClasses = 2\n",
    "batchSize = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/cpu:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 15567104157979966545\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Bidirectional, TimeDistributed, Dropout, Embedding\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "def auc(y_true, y_pred):\n",
    "     auc = tf.metrics.auc(y_true, y_pred)[1]\n",
    "     K.get_session().run(tf.local_variables_initializer())\n",
    "     return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 250, 50)           11198300  \n",
      "_________________________________________________________________\n",
      "LSTM (LSTM)                  (None, 100)               60400     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "softmax_output (Dense)       (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 11,258,902\n",
      "Trainable params: 60,602\n",
      "Non-trainable params: 11,198,300\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(\n",
    "    embeddingMatrix.shape[0],\n",
    "    embeddingMatrix.shape[1],\n",
    "    weights=[embeddingMatrix],\n",
    "    input_length=maxSeqLength,\n",
    "    trainable=False))\n",
    "\n",
    "model.add(LSTM(lstmUnits[0], name=\"LSTM\"))\n",
    "\n",
    "model.add(Dropout(.2, name=\"dropout\"))\n",
    "\n",
    "model.add(Dense(\n",
    "    2, \n",
    "    activation=\"softmax\", \n",
    "    name=\"softmax_output\"))\n",
    "\n",
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\", \n",
    "    optimizer=\"adam\", \n",
    "    metrics=[\"accuracy\", auc])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.set_random_seed(43245)\n",
    "np.random.seed(453252)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mariosk/anaconda2/envs/ipykernel_py3/lib/python3.6/site-packages/keras/models.py:944: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "10000/10000 [==============================] - 46s 5ms/step - loss: 0.4733 - acc: 0.7681 - auc: 0.6317 - val_loss: 0.3694 - val_acc: 0.9017 - val_auc: 0.8784\n",
      "Epoch 2/2\n",
      "10000/10000 [==============================] - 45s 5ms/step - loss: 0.3491 - acc: 0.9022 - auc: 0.8920 - val_loss: 0.3217 - val_acc: 0.9024 - val_auc: 0.8964\n",
      "CPU times: user 5min 18s, sys: 17.4 s, total: 5min 36s\n",
      "Wall time: 1min 32s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff30c218a20>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(\n",
    "    trainSentences, \n",
    "    trainLabels, \n",
    "    nb_epoch=2, \n",
    "    batch_size=batchSize,\n",
    "    validation_data=(testSentences, testLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(testSentences, batch_size=batchSize)\n",
    "prdVec = np.where(preds[:, 1] > 0.5, 1, 0)\n",
    "labs = testLabels[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate_predictions import evaluatePredictions\n",
    "\n",
    "evaluatePredictions(pd.Series(labs), prdVec, preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}